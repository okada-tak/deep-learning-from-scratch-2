{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWzKmAKuqKVvSoYzH5bnfT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/okada-tak/deep-learning-from-scratch-2/blob/master/notebooks/ch04.ipynb)\n",
        "\n",
        "# 4章 word2vecの高速化 のまとめ\n",
        "- Embeddedレイヤは単語の分散表現を格納し、順伝播において該当する単語IDのベクトルを抽出する\n",
        "- word2vecでは語彙数の増加に比例して計算量が増加するので、近似計算を行う高速な手法を使うとよい\n",
        "- Negative Samplingは負例をいくつかサンプリングする手法であり、これを利用すれば多値分類を二値分類として扱うことができる\n",
        "- word2vecによって得られた単語の分散表現は、単語の意味が埋め込まれたものであり、似たコンテキストで使われる単語は単語ベクトルの空間上で近い場所に位置するようになる\n",
        "- word2vecの単語の分散表現は、類推問題をベクトルの加算と減算によって解ける性質を持つ\n",
        "- word2vecは転移学習の点で特に重要であり、その単語の分散表現はさまざまな自然言語処理のタスクに利用できる"
      ],
      "metadata": {
        "id": "xn5qweAY19OD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 word2vecの改良①\n",
        "ボトルネック：  \n",
        "- 入力層のone-hot表現と重み行列$\\text{W}_\\text{in}$の積による計算（4.1節で解決）\n",
        "- 中間層と重み行列$\\text{W}_\\text{out}$の積およびSoftmaxレイヤの計算（4.2節で解決）\n",
        "\n"
      ],
      "metadata": {
        "id": "4U_cTvSA_fSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.1 Embeddingレイヤ\n",
        "重みパラメータから「単語IDに該当する行（ベクトル）」を抜き出すレイヤ：Embdddingレイヤ  \n",
        "単語の分散表現が格納される"
      ],
      "metadata": {
        "id": "9EAMgNYF1tBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.2 Embeddingレイヤの実装\n"
      ],
      "metadata": {
        "id": "9WuWSYLi2PZT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfrbG2_y-KBF",
        "outputId": "17842d50-3932-4b5f-b410-fec4b6f5faa0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  1,  2],\n",
              "       [ 3,  4,  5],\n",
              "       [ 6,  7,  8],\n",
              "       [ 9, 10, 11],\n",
              "       [12, 13, 14],\n",
              "       [15, 16, 17],\n",
              "       [18, 19, 20]])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "W = np.arange(21).reshape(7,3)\n",
        "W"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wrvHA2_2ix1",
        "outputId": "c07094a3-3739-4199-aff5-514822acedca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 7, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jECj-pG-2qpS",
        "outputId": "6a0ea540-d725-4924-edd3-07923b88dc1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15, 16, 17])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.array([1,0,3,0])\n",
        "W[idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pB-ZYNn2sCJ",
        "outputId": "45c621b2-a585-4e66-9bb1-47babf490a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  4,  5],\n",
              "       [ 0,  1,  2],\n",
              "       [ 9, 10, 11],\n",
              "       [ 0,  1,  2]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "common/layers.py"
      ],
      "metadata": {
        "id": "jKWpt3f529Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# p.136-137\n",
        "class Embedding:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.idx = None\n",
        "\n",
        "    def forward(self, idx):\n",
        "        W, = self.params\n",
        "        self.idx = idx\n",
        "        out = W[idx]\n",
        "        return out\n",
        "\n",
        "# p.138\n",
        "#    def backward(self, dout):\n",
        "#        dW, = self.grads\n",
        "#        dW[...] = 0\n",
        "#        dW[self.idx] = dout     # 悪い例\n",
        "#        return None\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dW, = self.grads\n",
        "        dW[...] = 0\n",
        "#        if GPU:\n",
        "#            np.scatter_add(dW, self.idx, dout)\n",
        "#        else:\n",
        "        np.add.at(dW, self.idx, dout)\n",
        "        return None"
      ],
      "metadata": {
        "id": "YIvDBWIs22UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 word2vecの改良②\n",
        "Negative Samplingを導入する\n",
        "\n",
        "### 4.2.1 中間層以降の計算の問題点\n",
        "- 中間層のニューロンと重み行列$\\text{W}_\\text{out}$の積\n",
        "- Softmaxレイヤの計算\n",
        "\n",
        "今回の例でのSoftmaxの式：  \n",
        "$$\n",
        "y_k=\\frac{\\exp(s_k)}{\\displaystyle \\sum_{i=1}^{1000000}\\exp(s_i)}\n",
        "$$\n",
        "\n",
        "### 4.2.2 多値分類から二値分類へ\n",
        "「コンテキストが『you』と『goodbye』のとき、ターゲットとなる単語は何ですか？」  \n",
        "↓  \n",
        "「コンテキストが『you』と『goodbye』のとき、ターゲットとなる単語は『say』ですか？」  \n",
        "\n",
        "出力側の重み$\\text{W}_\\text{out}$では、各単語IDの単語ベクトルが各列に格納されている。  \n",
        "『say』の単語ベクトルを抽出し、中間層のニューロンとの内積を求める。  \n",
        "$$\n",
        "(1 \\times 100) \\cdot (100 \\times 1) = (1 \\times 1)\n",
        "$$\n",
        "\n",
        "### 4.2.3 シグモイド関数と交差エントロピー誤差\n",
        "二値分類：スコアにシグモイド関数を適用して確率を得る。損失関数は「交差エントロピー誤差」  \n",
        "（多値分類：スコアにソフトマックス関数を適用して確率を得る。損失関数は同じく「交差エントロピー誤差」）  \n",
        "\n",
        "### 4.2.4 多値分類から二値分類へ（実装編）\n",
        "Embeddingレイヤと「dot演算（内積）」の処理を合わせたEmbedding Dotレイヤを導入する。  \n"
      ],
      "metadata": {
        "id": "J29EoGHa4jJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MnV4pu_ssboI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■Colaboratory用\n",
        "Google Colaboratoryの場合、Google Driveに  \n",
        "dl-from-scratch-2/ch04  \n",
        "というフォルダを用意し、そこにこのjupyter notebookを配置。  \n",
        "(dl-from-scratch-2の部分は任意。)  \n",
        "また、datasetフォルダとcommonフォルダを\n",
        "dl-from-scratch-2/dataset  \n",
        "dl-from-scratch-2/common\n",
        "にコピーしておく。  \n",
        "\n",
        "以下のセルでGoogle Driveをマウント。許可を求められるので許可する。"
      ],
      "metadata": {
        "id": "bAN74CQGlIq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8fAZVOfsfLH",
        "outputId": "ee32b4d6-0a51-4e40-a0ff-d2fd5b4d9951"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■Colaboratory用\n",
        "chdirする。"
      ],
      "metadata": {
        "id": "ls5on52nshBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys,os\n",
        "os.chdir('/content/drive/My Drive/dl-from-scratch-2/ch04')\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RZTiwEgksmle",
        "outputId": "b6488ae0-1db5-4fce-830b-032ffe4655ff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/dl-from-scratch-2/ch04'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ch04/negative_sampling_layer.py"
      ],
      "metadata": {
        "id": "oaLVKXsCr7Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *  # import numpy as np\n",
        "from common.layers import Embedding, SigmoidWithLoss\n",
        "import collections\n",
        "\n",
        "\n",
        "class EmbeddingDot:\n",
        "    def __init__(self, W):\n",
        "        self.embed = Embedding(W)\n",
        "        self.params = self.embed.params\n",
        "        self.grads = self.embed.grads\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, h, idx):\n",
        "        target_W = self.embed.forward(idx)\n",
        "        out = np.sum(target_W * h, axis=1)\n",
        "\n",
        "        self.cache = (h, target_W)\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        h, target_W = self.cache\n",
        "        dout = dout.reshape(dout.shape[0], 1)\n",
        "\n",
        "        dtarget_W = dout * h\n",
        "        self.embed.backward(dtarget_W)\n",
        "        dh = dout * target_W\n",
        "        return dh"
      ],
      "metadata": {
        "id": "3mpOwa4h4iO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.5 Negative Sampling\n",
        "ネガティブな例（負例）を少数サンプリングして学習する。  \n",
        "正例とサンプリングされた負例における損失を足し合わせて最終的な損失とする。\n",
        "\n",
        "### 4.2.6 Negative Samplingのサンプリング手法\n"
      ],
      "metadata": {
        "id": "YnQ2ThAY3AGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.choice(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY-e1yLxf631",
        "outputId": "6b3baf1f-3d95-4deb-a82f-4a205a26b46c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# もう1回\n",
        "np.random.choice(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aybzjTaf-jk",
        "outputId": "48e76f9f-31f0-4209-e684-c7586a577098"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['you', 'say', 'goodbye', 'I', 'hello', '.']\n",
        "np.random.choice(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GKIckGrdgKd0",
        "outputId": "5be7e2e6-2b49-4e52-c2f2-fce373ee60a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.choice(words, size=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izRs5J0YgUpT",
        "outputId": "be36de06-522d-45df-b171-a7502ee1f7ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['say', 'say', 'you', 'hello', 'goodbye'], dtype='<U7')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.choice(words, size=5, replace=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00r4G5RUgeMd",
        "outputId": "e941ddc2-e3a2-4e6d-9783-e08b256a9729"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['I', 'hello', '.', 'goodbye', 'say'], dtype='<U7')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
        "np.random.choice(words, p=p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_p55-927gjjc",
        "outputId": "b360695d-fe64-4e24-a57e-8d524d82394c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative samplingの式：\n",
        "$$\n",
        "P'(w_i)=\\frac{P(w_i)^{0.75}}{\\displaystyle \\sum_{j}^{n}P(w_j)^{0.75}}\n",
        "$$"
      ],
      "metadata": {
        "id": "nO7xOswBg0sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = [0.7, 0.29, 0.01]\n",
        "new_p = np.power(p, 0.75)\n",
        "new_p /=np.sum(new_p)\n",
        "print(new_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WAsxxw4gr3t",
        "outputId": "bcfc05dc-e2c9-4b18-e08a-39aa8ce95af5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.64196878 0.33150408 0.02652714]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ch04/negative_sampling_layer.py"
      ],
      "metadata": {
        "id": "4peuP_XziAVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *  # import numpy as np\n",
        "from common.layers import Embedding, SigmoidWithLoss\n",
        "import collections\n",
        "\n",
        "class UnigramSampler:\n",
        "    def __init__(self, corpus, power, sample_size):\n",
        "        self.sample_size = sample_size\n",
        "        self.vocab_size = None\n",
        "        self.word_p = None\n",
        "\n",
        "        counts = collections.Counter()\n",
        "        for word_id in corpus:\n",
        "            counts[word_id] += 1\n",
        "\n",
        "        vocab_size = len(counts)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.word_p = np.zeros(vocab_size)\n",
        "        for i in range(vocab_size):\n",
        "            self.word_p[i] = counts[i]\n",
        "\n",
        "        self.word_p = np.power(self.word_p, power)\n",
        "        self.word_p /= np.sum(self.word_p)\n",
        "\n",
        "    def get_negative_sample(self, target):\n",
        "        batch_size = target.shape[0]\n",
        "\n",
        "        if not GPU:\n",
        "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                p = self.word_p.copy()\n",
        "                target_idx = target[i]\n",
        "                p[target_idx] = 0\n",
        "                p /= p.sum()\n",
        "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
        "        else:\n",
        "            # GPU(cupy）で計算するときは、速度を優先\n",
        "            # 負例にターゲットが含まれるケースがある\n",
        "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
        "                                               replace=True, p=self.word_p)\n",
        "\n",
        "        return negative_sample"
      ],
      "metadata": {
        "id": "G3BgWiqFh1CP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = np.array([0,1,2,3,4,1,2,3])\n",
        "power = 0.75\n",
        "sample_size = 2\n",
        "\n",
        "sampler = UnigramSampler(corpus, power, sample_size)\n",
        "target = np.array([1,3,0])\n",
        "negative_sample = sampler.get_negative_sample(target)\n",
        "print(negative_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeNCcEUyjGNw",
        "outputId": "6f1c2b2d-1a4a-4f6b-d59d-9cbe268f6b30"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 3]\n",
            " [0 2]\n",
            " [2 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.7 Negative Samplingの実装\n"
      ],
      "metadata": {
        "id": "rI75Kzu_jiRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ch04/negative_sampling_layer.py"
      ],
      "metadata": {
        "id": "66uISt0CkDZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NegativeSamplingLoss:\n",
        "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
        "        self.sample_size = sample_size\n",
        "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
        "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
        "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.embed_dot_layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, h, target):\n",
        "        batch_size = target.shape[0]\n",
        "        negative_sample = self.sampler.get_negative_sample(target)\n",
        "\n",
        "        # 正例のフォワード\n",
        "        score = self.embed_dot_layers[0].forward(h, target)\n",
        "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
        "        loss = self.loss_layers[0].forward(score, correct_label)\n",
        "\n",
        "        # 負例のフォワード\n",
        "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
        "        for i in range(self.sample_size):\n",
        "            negative_target = negative_sample[:, i]\n",
        "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
        "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dh = 0\n",
        "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "            dscore = l0.backward(dout)\n",
        "            dh += l1.backward(dscore)\n",
        "\n",
        "        return dh"
      ],
      "metadata": {
        "id": "gqrlaYA7jdBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 改良版word2vecの学習\n",
        "### 4.3.1 CBOWモデルの実装"
      ],
      "metadata": {
        "id": "G_4onQcelSK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ch04/cbow.py"
      ],
      "metadata": {
        "id": "IZwdPvOTlZBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *  # import numpy as np\n",
        "from common.layers import Embedding\n",
        "from ch04.negative_sampling_layer import NegativeSamplingLoss\n",
        "\n",
        "\n",
        "class CBOW:\n",
        "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
        "        V, H = vocab_size, hidden_size\n",
        "\n",
        "        # 重みの初期化\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
        "\n",
        "        # レイヤの生成\n",
        "        self.in_layers = []\n",
        "        for i in range(2 * window_size):\n",
        "            layer = Embedding(W_in)  # Embeddingレイヤを使用\n",
        "            self.in_layers.append(layer)\n",
        "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
        "\n",
        "        # すべての重みと勾配をリストにまとめる\n",
        "        layers = self.in_layers + [self.ns_loss]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # メンバ変数に単語の分散表現を設定\n",
        "        self.word_vecs = W_in\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        h = 0\n",
        "        for i, layer in enumerate(self.in_layers):\n",
        "            h += layer.forward(contexts[:, i])\n",
        "        h *= 1 / len(self.in_layers)\n",
        "        loss = self.ns_loss.forward(h, target)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.ns_loss.backward(dout)\n",
        "        dout *= 1 / len(self.in_layers)\n",
        "        for layer in self.in_layers:\n",
        "            layer.backward(dout)\n",
        "        return None"
      ],
      "metadata": {
        "id": "nWnegn7rlb2e"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.2 CBOWモデルの学習コード\n"
      ],
      "metadata": {
        "id": "delZSdHul0aC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ch04/train.py  \n",
        "長時間かかるようなのでとりあえずパス。"
      ],
      "metadata": {
        "id": "wQpBww5vl4Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common import config\n",
        "# GPUで実行する場合は、下記のコメントアウトを消去（要cupy）\n",
        "# ===============================================\n",
        "# config.GPU = True\n",
        "# ===============================================\n",
        "from common.np import *\n",
        "import pickle\n",
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "from cbow import CBOW\n",
        "from skip_gram import SkipGram\n",
        "from common.util import create_contexts_target, to_cpu, to_gpu\n",
        "from dataset import ptb\n",
        "\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "window_size = 5\n",
        "hidden_size = 100\n",
        "batch_size = 100\n",
        "max_epoch = 10\n",
        "\n",
        "# データの読み込み\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "if config.GPU:\n",
        "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
        "\n",
        "# モデルなどの生成\n",
        "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
        "# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "# 学習開始\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()\n",
        "\n",
        "# 後ほど利用できるように、必要なデータを保存\n",
        "word_vecs = model.word_vecs\n",
        "if config.GPU:\n",
        "    word_vecs = to_cpu(word_vecs)\n",
        "params = {}\n",
        "params['word_vecs'] = word_vecs.astype(np.float16)\n",
        "params['word_to_id'] = word_to_id\n",
        "params['id_to_word'] = id_to_word\n",
        "pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\n",
        "with open(pkl_file, 'wb') as f:\n",
        "    pickle.dump(params, f, -1)"
      ],
      "metadata": {
        "id": "DEZFqMHrl50d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.3 CBOWモデルの評価"
      ],
      "metadata": {
        "id": "4ufiZkJtmTQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ch04/eval.py"
      ],
      "metadata": {
        "id": "PpNFDqoRmWUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.util import most_similar, analogy\n",
        "import pickle\n",
        "\n",
        "\n",
        "pkl_file = 'cbow_params.pkl'\n",
        "# pkl_file = 'skipgram_params.pkl'\n",
        "\n",
        "with open(pkl_file, 'rb') as f:\n",
        "    params = pickle.load(f)\n",
        "    word_vecs = params['word_vecs']\n",
        "    word_to_id = params['word_to_id']\n",
        "    id_to_word = params['id_to_word']\n",
        "\n",
        "# most similar task\n",
        "querys = ['you', 'year', 'car', 'toyota']\n",
        "for query in querys:\n",
        "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s84FV7eymXtI",
        "outputId": "e4f0ce43-6d60-460f-cbd9-28a218716e3b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[query] you\n",
            " we: 0.6103515625\n",
            " someone: 0.59130859375\n",
            " i: 0.55419921875\n",
            " something: 0.48974609375\n",
            " anyone: 0.47314453125\n",
            "\n",
            "[query] year\n",
            " month: 0.71875\n",
            " week: 0.65234375\n",
            " spring: 0.62744140625\n",
            " summer: 0.6259765625\n",
            " decade: 0.603515625\n",
            "\n",
            "[query] car\n",
            " luxury: 0.497314453125\n",
            " arabia: 0.47802734375\n",
            " auto: 0.47119140625\n",
            " disk-drive: 0.450927734375\n",
            " travel: 0.4091796875\n",
            "\n",
            "[query] toyota\n",
            " ford: 0.55078125\n",
            " instrumentation: 0.509765625\n",
            " mazda: 0.49365234375\n",
            " bethlehem: 0.47509765625\n",
            " nissan: 0.474853515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "common/util.py"
      ],
      "metadata": {
        "id": "2oaDcBs6n3Ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
        "    for word in (a, b, c):\n",
        "        if word not in word_to_id:\n",
        "            print('%s is not found' % word)\n",
        "            return\n",
        "\n",
        "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
        "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
        "    query_vec = b_vec - a_vec + c_vec\n",
        "    query_vec = normalize(query_vec)\n",
        "\n",
        "    similarity = np.dot(word_matrix, query_vec)\n",
        "\n",
        "    if answer is not None:\n",
        "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
        "\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if np.isnan(similarity[i]):\n",
        "            continue\n",
        "        if id_to_word[i] in (a, b, c):\n",
        "            continue\n",
        "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return\n",
        "\n",
        "def normalize(x):\n",
        "    if x.ndim == 2:\n",
        "        s = np.sqrt((x * x).sum(1))\n",
        "        x /= s.reshape((s.shape[0], 1))\n",
        "    elif x.ndim == 1:\n",
        "        s = np.sqrt((x * x).sum())\n",
        "        x /= s\n",
        "    return x"
      ],
      "metadata": {
        "id": "QJvukmydm67c"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW7WhGmuoCAO",
        "outputId": "66f4fff7-e9bf-4a33-f233-050dc655460e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[analogy] king:man = queen:?\n",
            " woman: 5.16015625\n",
            " veto: 4.9296875\n",
            " ounce: 4.69140625\n",
            " earthquake: 4.6328125\n",
            " successor: 4.609375\n",
            "\n",
            "[analogy] take:took = go:?\n",
            " went: 4.55078125\n",
            " points: 4.25\n",
            " began: 4.09375\n",
            " comes: 3.98046875\n",
            " oct.: 3.90625\n",
            "\n",
            "[analogy] car:cars = child:?\n",
            " children: 5.21875\n",
            " average: 4.7265625\n",
            " yield: 4.20703125\n",
            " cattle: 4.1875\n",
            " priced: 4.1796875\n",
            "\n",
            "[analogy] good:better = bad:?\n",
            " more: 6.6484375\n",
            " less: 6.0625\n",
            " rather: 5.21875\n",
            " slower: 4.734375\n",
            " greater: 4.671875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 word2vecに関する残りのテーマ\n",
        "### 4.4.1 word2vecを使ったアプリケーションの例\n",
        "転移学習\n",
        "\n",
        "### 4.4.2 単語ベクトルの評価方法\n",
        "類似性：人が出した類似度のスコアとword2vecによるコサイン類似度のスコアを比較して相関性をみる  \n",
        "類推問題：「king : queen = man : ?」のような類推問題を出題し、その正解率をみる  \n",
        "\n",
        "- モデルによって精度が異なる（コーパスに応じて最適なモデルを選ぶ）\n",
        "- コーパスが大きいほど良い結果になる（ビッグデータは常に望まれる）\n",
        "- 単語ベクトルの次元数は適度な大きさが必要（大きすぎても精度が悪くなる）"
      ],
      "metadata": {
        "id": "XBXpVNp8sc4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 まとめ\n"
      ],
      "metadata": {
        "id": "t7Yc0MjZujLI"
      }
    }
  ]
}