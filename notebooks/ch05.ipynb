{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5YCEjlDTthCeveZ3ZIq+Y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/okada-tak/deep-learning-from-scratch-2/blob/master/notebooks/ch05.ipynb)\n",
        "\n",
        "# 5章 リカレントニューラルネットワーク(RNN) のまとめ\n",
        "- RNNはループする経路があり、それによって「隠れ状態」を内部に記憶することができる\n",
        "- RNNのループ経路を展開することで、複数のRNNレイヤがつながったネットワークと解釈することができ、通常の誤差逆伝播法によって学習することができる（＝BPTT）\n",
        "- 長い時系列データを学習する場合は、適当な長さでデータのまとまりを作り－これを「ブロック」という－、ブロック単位でBPTTによる学習を行う（＝Truncated BPTT）\n",
        "- Truncated BPTTでは逆伝播のつながりのみを切断する\n",
        "- Truncated BPTTでは順伝播のつながりは維持するため、データは”シーケンシャル”に与える必要がある\n",
        "- 言語モデルは、単語の羅列を確率として解釈する\n",
        "- RNNレイヤを利用した条件付き言語モデルは、（理論的には）それまで登場した単語の情報を記憶することができる"
      ],
      "metadata": {
        "id": "6aj0HdsWDbmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "単純なフィードフォワード・ネットワークでは、時系列データの性質（パターン）を十分に学習することができない"
      ],
      "metadata": {
        "id": "uo_8iWY6E2tU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 確率と言語モデル"
      ],
      "metadata": {
        "id": "iR0ueYanFAKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.1 word2vecを確率の視点から眺める\n",
        "$w_{t-1}$と$w_{t+1}$が与えられたとき、ターゲットが$w_t$となる確率  \n",
        "$$\n",
        "P(w_t|w_{t-1}, w_{t+1})\n",
        "$$\n",
        "\n",
        "左側2つの単語だけをコンテキストとして考える\n",
        "$$\n",
        "P(w_t|w_{t-2}, w_{t-1})\n",
        "$$\n",
        "\n",
        "CBOWモデルが扱う損失関数\n",
        "$$\n",
        "L=-logP(w_t|w_{t-2}, w_{t-1})\n",
        "$$"
      ],
      "metadata": {
        "id": "LXQSyax2VDvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.2 言語モデル\n",
        "$w_1$,…,$w_m$の$m$個の単語からなる文章  \n",
        "$w_1$,…,$w_m$という順序で単語が出現する確率  \n",
        "$$\n",
        "\\begin{split}\n",
        "P(w_1,…,w_m)&=P(w_m|w_1,…,w_{m-1})P(w_{m-1}|w_1,…,w_{m-2})...P(w_3|w_1,w_2)P(w_2|w_1)p(w_1)\\\\  \n",
        "&=\\prod_{t=1}^mP(w_t|w_1,…,w_{t-1})\n",
        "\\end{split}\n",
        "$$\n",
        "目標：  \n",
        "$P(w_t|w_1,…,w_{t-1})$を求めること。これがわかれば言語モデルの同時確率$P(w_1,…,w_m)$を求められる"
      ],
      "metadata": {
        "id": "O1vKl3ctWXET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.3 CBOWモデルを言語モデルに？\n",
        "コンテキストのサイズを限定することでCBOWモデルを言語モデルに適用\n",
        "$$\n",
        "P(w_1,…,w_m)=\\prod_{t=1}^mP(w_t|w_1,…,w_{t-1})≈\\prod_{t=1}^mP(w_t|w_{t-2},w_{t-1})\n",
        "$$\n",
        "直前の2つの単語だけに依存するとして2階のマルコフ連鎖  \n",
        "CBOWモデルではコンテキスト内の単語の並びが無視されるのが問題\n"
      ],
      "metadata": {
        "id": "wMhg_k8QVJ2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 RNNとは"
      ],
      "metadata": {
        "id": "acOki6p_cSNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.1 循環するニューラルネットワーク\n",
        "### 5.2.2 ループの展開\n",
        "$$\n",
        "\\mathbf h_t=tanh(\\mathbf h_{t-1}\\mathbf W_\\mathbf h+\\mathbf x_t\\mathbf W_\\mathbf x+\\mathbf b)\n",
        "$$"
      ],
      "metadata": {
        "id": "7uEj240ice4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.3 Backpropagation Through Time\n",
        "長い時系列データを扱うと消費する計算リソースが増大する  \n",
        "逆伝播時の勾配が不安定になる"
      ],
      "metadata": {
        "id": "-B6eImjGewDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.4 Truncated BPTT\n",
        "ネットワークの逆伝播のつながりだけを適当な長さで切り取り、切り取られたネットワーク単位で学習する\n",
        "\n",
        "順伝播のつながりは切断しないのでデータをシーケンシャルに与える必要がある"
      ],
      "metadata": {
        "id": "5TXlXCXlgyPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.5 Truncated BPTTのミニバッチ学習\n",
        "データの与え方：\n",
        "\n",
        "*   データをシーケンシャルに与えること\n",
        "*   各バッチでデータを与える開始位置をずらすこと\n",
        "\n"
      ],
      "metadata": {
        "id": "rxYOnm28h-nC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 RNNの実装"
      ],
      "metadata": {
        "id": "c2QaGBlqi6DC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ffWPuunlErEU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}