{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLQbAnClV2+/LzZ/trz6bi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/okada-tak/deep-learning-from-scratch-2/blob/master/notebooks/ch05.ipynb)\n",
        "\n",
        "# 5章 リカレントニューラルネットワーク(RNN) のまとめ\n",
        "- RNNはループする経路があり、それによって「隠れ状態」を内部に記憶することができる\n",
        "- RNNのループ経路を展開することで、複数のRNNレイヤがつながったネットワークと解釈することができ、通常の誤差逆伝播法によって学習することができる（＝BPTT）\n",
        "- 長い時系列データを学習する場合は、適当な長さでデータのまとまりを作り－これを「ブロック」という－、ブロック単位でBPTTによる学習を行う（＝Truncated BPTT）\n",
        "- Truncated BPTTでは逆伝播のつながりのみを切断する\n",
        "- Truncated BPTTでは順伝播のつながりは維持するため、データは”シーケンシャル”に与える必要がある\n",
        "- 言語モデルは、単語の羅列を確率として解釈する\n",
        "- RNNレイヤを利用した条件付き言語モデルは、（理論的には）それまで登場した単語の情報を記憶することができる"
      ],
      "metadata": {
        "id": "6aj0HdsWDbmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "単純なフィードフォワード・ネットワークでは、時系列データの性質（パターン）を十分に学習することができない"
      ],
      "metadata": {
        "id": "uo_8iWY6E2tU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 確率と言語モデル"
      ],
      "metadata": {
        "id": "iR0ueYanFAKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.1 word2vecを確率の視点から眺める\n",
        "$w_{t-1}$と$w_{t+1}$が与えられたとき、ターゲットが$w_t$となる確率  \n",
        "$$\n",
        "P(w_t|w_{t-1}, w_{t+1})\n",
        "$$\n",
        "\n",
        "左側2つの単語だけをコンテキストとして考える\n",
        "$$\n",
        "P(w_t|w_{t-2}, w_{t-1})\n",
        "$$\n",
        "\n",
        "CBOWモデルが扱う損失関数\n",
        "$$\n",
        "L=-logP(w_t|w_{t-2}, w_{t-1})\n",
        "$$"
      ],
      "metadata": {
        "id": "LXQSyax2VDvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.2 言語モデル\n",
        "$w_1$,…,$w_m$の$m$個の単語からなる文章  \n",
        "$w_1$,…,$w_m$という順序で単語が出現する確率  \n",
        "$$\n",
        "\\begin{split}\n",
        "P(w_1,…,w_m)&=P(w_m|w_1,…,w_{m-1})P(w_{m-1}|w_1,…,w_{m-2})...P(w_3|w_1,w_2)P(w_2|w_1)p(w_1)\\\\  \n",
        "&=\\prod_{t=1}^mP(w_t|w_1,…,w_{t-1})\n",
        "\\end{split}\n",
        "$$\n",
        "目標：  \n",
        "$P(w_t|w_1,…,w_{t-1})$を求めること。これがわかれば言語モデルの同時確率$P(w_1,…,w_m)$を求められる"
      ],
      "metadata": {
        "id": "O1vKl3ctWXET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.3 CBOWモデルを言語モデルに？\n",
        "コンテキストのサイズを限定することでCBOWモデルを言語モデルに適用\n",
        "$$\n",
        "P(w_1,…,w_m)=\\prod_{t=1}^mP(w_t|w_1,…,w_{t-1})≈\\prod_{t=1}^mP(w_t|w_{t-2},w_{t-1})\n",
        "$$\n",
        "直前の2つの単語だけに依存するとして2階のマルコフ連鎖  \n",
        "CBOWモデルではコンテキスト内の単語の並びが無視されるのが問題\n"
      ],
      "metadata": {
        "id": "wMhg_k8QVJ2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 RNNとは"
      ],
      "metadata": {
        "id": "acOki6p_cSNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.1 循環するニューラルネットワーク\n",
        "### 5.2.2 ループの展開\n",
        "$$\n",
        "\\mathbf h_t=tanh(\\mathbf h_{t-1}\\mathbf W_\\mathbf h+\\mathbf x_t\\mathbf W_\\mathbf x+\\mathbf b)\n",
        "$$"
      ],
      "metadata": {
        "id": "7uEj240ice4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.3 Backpropagation Through Time\n",
        "長い時系列データを扱うと消費する計算リソースが増大する  \n",
        "逆伝播時の勾配が不安定になる"
      ],
      "metadata": {
        "id": "-B6eImjGewDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.4 Truncated BPTT\n",
        "ネットワークの逆伝播のつながりだけを適当な長さで切り取り、切り取られたネットワーク単位で学習する\n",
        "\n",
        "順伝播のつながりは切断しないのでデータをシーケンシャルに与える必要がある"
      ],
      "metadata": {
        "id": "5TXlXCXlgyPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.5 Truncated BPTTのミニバッチ学習\n",
        "データの与え方：\n",
        "\n",
        "*   データをシーケンシャルに与えること\n",
        "*   各バッチでデータを与える開始位置をずらすこと\n",
        "\n"
      ],
      "metadata": {
        "id": "rxYOnm28h-nC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 RNNの実装\n",
        "RNNの1ステップの処理を行うクラスをRNNクラス\n",
        "\n",
        "そのRNNクラスを利用して、Tステップの処理をまとめて行うレイヤをTimeRNNクラス"
      ],
      "metadata": {
        "id": "c2QaGBlqi6DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3.1 RNNレイヤの実装\n",
        "RNNの順伝播  \n",
        "$$\n",
        "\\mathbf h_t=tanh(\\mathbf h_{t-1}\\mathbf W_\\mathbf h+\\mathbf x_t\\mathbf W_\\mathbf x+\\mathbf b)\n",
        "$$"
      ],
      "metadata": {
        "id": "CH6e44tPX9g-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■Colaboratory用\n",
        "Google Colaboratoryの場合、Google Driveに  \n",
        "dl-from-scratch-2/ch05  \n",
        "というフォルダを用意し、そこにこのjupyter notebookを配置。  \n",
        "(dl-from-scratch-2の部分は任意。)  \n",
        "また、datasetフォルダとcommonフォルダを\n",
        "dl-from-scratch-2/dataset  \n",
        "dl-from-scratch-2/common\n",
        "にコピーしておく。  \n",
        "\n",
        "以下のセルでGoogle Driveをマウント。許可を求められるので許可する。"
      ],
      "metadata": {
        "id": "1YEjgSkFd2v0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hoW5VDk1d8m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ■Colaboratory用\n",
        "chdirする。"
      ],
      "metadata": {
        "id": "IaYiyQXqeVgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys,os\n",
        "os.chdir('/content/drive/My Drive/dl-from-scratch-2/ch05')\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "X9EzueHEd9Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "common/time_layers.py"
      ],
      "metadata": {
        "id": "RrqUViDma8pM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from common.np import *  # import numpy as np (or import cupy as np)\n",
        "from common.layers import *\n",
        "from common.functions import softmax, sigmoid\n",
        "\n",
        "class RNN:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        Wx, Wh, b = self.params\n",
        "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
        "        h_next = np.tanh(t)\n",
        "\n",
        "        self.cache = (x, h_prev, h_next)\n",
        "        return h_next\n",
        "\n",
        "    def backward(self, dh_next):\n",
        "        Wx, Wh, b = self.params\n",
        "        x, h_prev, h_next = self.cache\n",
        "\n",
        "        dt = dh_next * (1 - h_next ** 2)\n",
        "        db = np.sum(dt, axis=0)\n",
        "        dWh = np.dot(h_prev.T, dt)\n",
        "        dh_prev = np.dot(dt, Wh.T)\n",
        "        dWx = np.dot(x.T, dt)\n",
        "        dx = np.dot(dt, Wx.T)\n",
        "\n",
        "        self.grads[0][...] = dWx\n",
        "        self.grads[1][...] = dWh\n",
        "        self.grads[2][...] = db\n",
        "\n",
        "        return dx, dh_prev"
      ],
      "metadata": {
        "id": "ffWPuunlErEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3.2 Time RNNレイヤの実装\n"
      ],
      "metadata": {
        "id": "sYdpExkveuFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "common/time_layers.py"
      ],
      "metadata": {
        "id": "tZN9vHL0e5j9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeRNN:\n",
        "    def __init__(self, Wx, Wh, b, stateful=False):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.layers = None\n",
        "\n",
        "        self.h, self.dh = None, None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, D = xs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = RNN(*self.params)\n",
        "            self.h = layer.forward(xs[:, t, :], self.h)\n",
        "            hs[:, t, :] = self.h\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, H = dhs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        dh = 0\n",
        "        grads = [0, 0, 0]\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
        "            dxs[:, t, :] = dx\n",
        "\n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "\n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h):\n",
        "        self.h = h\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h = None"
      ],
      "metadata": {
        "id": "l4u3v9t3e9uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 時系列データを扱うレイヤの実装"
      ],
      "metadata": {
        "id": "T6bzkBgpgE-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4.1 RNNLMの全体図"
      ],
      "metadata": {
        "id": "VqAuSxR7gJXT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z0jxEMQHgIvq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}