{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO89MVe1xacwExkdjwC02ev"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDeTCkiaDS_5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/okada-tak/deep-learning-from-scratch-2/blob/master/notebooks/ch05.ipynb)\n",
        "\n",
        "# 5章 リカレントニューラルネットワーク(RNN) のまとめ\n",
        "- RNNはループする経路があり、それによって「隠れ状態」を内部に記憶することができる\n",
        "- RNNのループ経路を展開することで、複数のRNNレイヤがつながったネットワークと解釈することができ、通常の誤差逆伝播法によって学習することができる（＝BPTT）\n",
        "- 長い時系列データを学習する場合は、適当な長さでデータのまとまりを作り－これを「ブロック」という－、ブロック単位でBPTTによる学習を行う（＝Truncated BPTT）\n",
        "- Truncated BPTTでは逆伝播のつながりのみを切断する\n",
        "- Truncated BPTTでは順伝播のつながりは維持するため、データは”シーケンシャル”に与える必要がある\n",
        "- 言語モデルは、単語の羅列を確率として解釈する\n",
        "- RNNレイヤを利用した条件付き言語モデルは、（理論的には）それまで登場した単語の情報を記憶することができる"
      ],
      "metadata": {
        "id": "6aj0HdsWDbmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "単純なフィードフォワード・ネットワークでは、時系列データの性質（パターン）を十分に学習することができない"
      ],
      "metadata": {
        "id": "uo_8iWY6E2tU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 確率と言語モデル\n",
        "### 5.1.1 word2vecを確率の視点から眺める"
      ],
      "metadata": {
        "id": "iR0ueYanFAKl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ffWPuunlErEU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}